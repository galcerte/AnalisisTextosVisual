---
title: "Herramientas de visualización de características de textos"
subtitle: "Análisis de Señales  \n Máster en Ciencia de Datos  \n Universidad de Valencia"
author: 'Agustín Matías Galante Cerviño y Carlos Rábano Alcoba'
date: '`r format(Sys.time(), "%d/%m/%Y")`'
fontsize: 12pt
output:
  pdf_document:
    number_sections: yes
    toc: no
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

# Introducción
Visualizar los resultados de procesar una serie de datos es una de las tareas más importantes a
realizar para obtener conclusiones sobre éstos; dado que suelen dejar en evidencia con un simple
vistazo si existen posibles anomalías, si se sigue el comportamiento esperado, o si existen puntos
de interés para futuras investigaciones.

En este trabajo, no buscamos *adquirir* o *procesar* características de textos *per se*, sino
considerar las opciones al *visualizar* el resultado de éstas dos primeras acciones, así que no
haremos énfasis en éstos.

Tenemos varias características del texto que podemos utilizar para representar. La más inmediata en
que pensamos al considerar el tema de este trabajo es la **frecuencia de cada palabra**. Este es un
tipo de dato posible de adquirir de forma sencilla, y su visualización es fácilmente entendible a
través de un histograma.

Podemos recurrir también a representaciones más elaboradas y artísticas como las **nubes de
palabras**. En ellas se presentan las palabras tanto más grandes cuantas más veces aparecen en el
texto. La disposición de las mismas no tiene nada que ver con sus relaciones. Como ya seguramente
podamos intuir, estas representaciones son más artísticas que técticas o útiles, ya que su
interpretación puede ser confusa. En los histogramas de las palabras más frecuentes tenemos toda la
información (y más completa, porque tenemos información numérica en los ejes) que representan las
nubes de palabras.

Algo más avanzado sería analizar el **sentimiento del texto**, lo cual implicaría ya categorizar a
las distintas palabras apareciendo en éste. Ésto ya es una de las tareas de procesado del texto que
requieren no sólo de algoritmos más complicados, sino de aprendizaje máquina, así que será
imprescindible utilizar paquetes externos. Afortunadamente, hay lexicones (categorizaciones de
palabras) ya hechos mediante éstas técnicas, disponibles en paquetes como `tidytext`, los cuales nos
permiten obtener el sentimiento con sólo unir dicho lexicon con el *tibble* que contenga el texto.

También se pueden categorizar grupos de palabras, llamados **n-gramas**. Los más habituales son los
que analizan la frecuencia de aparición de pares de palabras o de trios clasificándolos según el
sentimiento.

Otra opción de visualización aun más compleja son son los **mapas de co-ocurrencia**. Estos se
contruyen a partir de una palabra y sus relaciones (apariciones junto a otras) a lo largo del texto.
Son palabras de tamaño variable (en función de las veces que aparecen) interconectadas por líneas
según sus relaciones. Centrándonos en el análisis de libros, un ejercicio interesante sería contruir
los mapas de co-ocurrencia de algunos personajes, para estudiar hasta qué punto estos pueden
representar de una forma más o menos fiel la realidad.

Para ejemplificar esta serie de visualizaciones introducidas estudiaremos dos libros distintos
del mismo autor, Charles Dickens. Estos son "Tiempos Difíciles" y "Grandes Esperanzas". Ambos podemos
importarlos descargándolos directamente desde R con la librería `gutenbergr`, en inglés.

El código usado para realizar esta memoria se encuentra en
[este repositorio de GitHub.](https://github.com/galcerte/AnalisisTextosVisual)

Para adquirir y analizar estos dos libros, haremos uso de los siguientes paquetes, además de
la siguiente lógica para comprobar que los tenemos instalados.

```{r}
# Especificamos las librerías necesarias en esta lista
packages = c(# Programación literaria; R Markdowns
             "knitr",
             # Descarga de libros de la biblioteca Gutenberg
             "gutenbergr",
             # Datos ordenados
             "tidyr",
             # Textos ordenados
             "tidytext",
             # Manipulación de datos
             "dplyr",
             # No necesitamos tidyverse ya que estamos incluyendo una
             # parte de los paquetes incluidos en este
             #"tidyverse",
             # Representaciones gráficas
             "ggplot2",
             # Nubes de palabras
             "wordcloud",
             "reshape2",
             "quanteda",
             # Redes de co-ocurrencia
             "igraph",
             "lexicon")

# Esta función se usa para comprobar si cada paquete está en la máquina local
# Si un paquete está instalado, será cargado
# Si alguno no lo está, el/los paquete/s ausente/s será/n instalado/s y cargado/s
package.check <- lapply(packages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})

# Verificamos que estén cargados
search()
```

# Tiempos Difíciles
## Organización de los datos
### Clasificación de las líneas por las partes del libro y por los capítulos
Como ya comentamos en la introducción, el objetivo del trabajo es estudiar las diversas
visualizaciones de características de textos. La importación y organización de los datos no es
relevante. No obstante, por si se quisiera repetir el análisis que hemos hecho, incluimos los todos
los comandos utilizados en el trabajo.

Para empezar, descargamos el libro y seleccionamos las líneas del texto (eliminamos lo que aparece
al principio, el índice y la portada, y lo que aparece al final, así como el título de los capítulos
y las partes del libro). Mientras tanto, vamos etiquetando cada línea con el capítulo al que
pertenece y la parte del libro (que de ahora en adelante llamaremos libro por simplificar). De esta
forma podremos luego realizar un análisis por libros o capítulos.

```{r}
# Buscamos en https://www.gutenberg.org/ el "EBook-No." del libro (786)
HardTimesRaw <- gutenberg_download(786)

# Encontramos el número de cada línea donde comienza un libro
BooksStart <- grep(pattern = "BOOK THE", x = HardTimesRaw$text) + 4
# y ahora donde termina,
BooksEnd <- grep(pattern = "(END OF THE|FOOTNOTES)", x = HardTimesRaw$text) - 4

# Eliminamos el contenido de las apariciones en el índice
BooksStart <- BooksStart[(length(BooksStart)/2 + 1):length(BooksStart)]

bookBooks <- numeric(length(HardTimesRaw$text))
textBooks <- character(length(HardTimesRaw$text))

# Dividimos en libros
m <- 1
for (i in 1:length(BooksStart)) {
  s <- BooksStart[i]
  e <- BooksEnd[i]
  bookBooks[m:(e - s + m)] <- i
  textBooks[m:(e - s + m)] <- HardTimesRaw$text[s:e]
  m <- m + e - s + 1
}

ChaptersStart <- vector("list", length = length(BooksStart))
ChaptersEnd <- vector("list", length = length(BooksStart))

# Principio y final de los capítulos
for (i in 1:length(BooksStart)) {
  s <- which(bookBooks == i)[1]
  ChaptersStart[[i]] <- grep(pattern = "CHAPTER", x = textBooks[bookBooks == i]) + s + 3
  ChaptersEnd[[i]] <- c(ChaptersStart[[i]][2:length(ChaptersStart[[i]])] - 8, BooksEnd[i])
}

# Creamos tres vectores con la longitud de las líneas del texto,
book <- numeric(length(HardTimesRaw$text))
chapter <- numeric(length(HardTimesRaw$text))
text <- character(length(HardTimesRaw$text))

# Dividimos en capítulos
m <- 1
for (i in 1:length(BooksStart)) {
  for (j in 1:length(ChaptersStart[[i]])) {
    s <- ChaptersStart[[i]][j]
    e <- ChaptersEnd[[i]][j]
    # En cada elemento de "book", se indica a qué libro
    # corresponde cada línea,
    book[m:(e - s + m)] <- i
    # En cada elemento de "chapter" se indica a qué capítulo
    # corresponde cada línea,
    chapter[m:(e - s + m)] <- j
    # Y en "text" van las líneas en sí.
    text[m:(e - s + m)] <- textBooks[s:e]
    m <- m + e - s + 1
  }
}

# Obtenemos el índice de la última línea del libro
e <- which(text == "cold.") 
HardTimesLines <- data.frame(book = book[1:e], chapter = chapter[1:e], text = text[1:e])
save(HardTimesLines, file = "data/HardTimesLines.Rdata")
```

### División por palabras
Es necesario también dividir el texto por palabras, si queremos contar las veces que aparece cada
una, por ejemplo. Al llevar a cabo este proceso hemos de asegurarnos de eliminar correctamente los
signos de puntuación. También hemos de recordar el uso muy común en inglés del apóstrofe "’".
Será mejor no eliminar este caracter, para conservar palabras como por ejemplo "don’t".

Otro detalle importante a tener en cuenta es que para contar las veces que aparece cada palabra no
nos interesa distinguir entre si aparece con la primera letra en mayúscula o no. Con lo cual
utilizaremos la función `tolower()` para pasar todas las letras de las palabras a minúsculas.

```{r}
WordsRaw <- vector("list", length = nrow(HardTimesLines))
Words <- vector("list", length = nrow(HardTimesLines))

book <- numeric(10*nrow(HardTimesLines))
chapter <- numeric(10*nrow(HardTimesLines))
line <- numeric(10*nrow(HardTimesLines))
word <- character(10*nrow(HardTimesLines))

n <- 0
for (i in 1:nrow(HardTimesLines)) {
  WordsRaw[[i]] <- strsplit(x = HardTimesLines$text[i], split = "[ .,;—)(?!‘:]")
  Words[[i]] <- WordsRaw[[i]][[1]][!WordsRaw[[i]][[1]] == ""]
  if (length(Words[[i]]) == 0) {
    next
  }
  l <- length(Words[[i]])
  n <- n + l
  book[(n - l + 1):n] <- HardTimesLines$book[i]
  chapter[(n - l + 1):n] <- HardTimesLines$chapter[i]
  line[(n - l + 1):n] <- i
  word[(n - l + 1):n] <- Words[[i]]
}

word <- tolower(word)
HardTimesWords <- data.frame(book = book[1:n], chapter = chapter[1:n], line = line[1:n], word = word[1:n])
HardTimesWords <- HardTimesWords[HardTimesWords$word != "’", ]
save(HardTimesWords, file = "data/HardTimesWords.Rdata")
```

## Visualización de características
### Palabras más frecuentes
Comencemos el análisis de características del libro por lo más sencillo, ver qué palabras aparecen
más veces a lo largo del texto (repeticiones de las palabras: variable `n`). Para facilitar el
trabajo recurrimos a la librería `dplyr`. Esta contiene un montón de funciones para el contaje,
filtrado y modificación de estructuras de datos. La librería `tidytext` nos proporciona un lexicon
con palabras vacías o *stop words* (palabras como *the* o *and*). Con estas dos librerías podemos
contar las apariciones de cada palabra en el texto y excluir aquellas que no aporten significado al
mismo. Veamos las 25 palabras más frecuentes en Tiempos Difíciles.

```{r}
WORDS <- count(HardTimesWords, word, sort = TRUE) %>%
  anti_join(get_stopwords())
head(WORDS, 25)
```

La forma visual de representar esta información es mediante un histograma. Para construir las
gráficas utilizaremos siempre la librería `ggplot2`, de las más potentes y conocida por todo el que
utilice R. Para hacer la representación filtramos primero las palabras que aparecen más veces
que un cierto umbral y luego las ordenamos según sus apariciones, variable `n`.

```{r}
WORDS %>% filter(n > 180) %>% mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + coord_flip() + labs(title = "Palabras más comunes en Tiempos Difíciles")
```

También podemos representar esta información mediante una nube de palabras. Estas son
representaciones gráficas en las que se disponen de forma arbitraria las palabras con un tamaño
variable en función de lo frecuentes que son en el texto. Como ya dijimos, son más bien una
representación artística, a nosotros como científicos de datos nos aporta mucha más información y
mas adecuada un histograma. Aun así, merece la pena contruir una para ver cómo son, la librería
`wordcloud` nos hace todo el trabajo.

```{r}
HardTimesWords %>%
  anti_join(stop_words) %>%
  count(word) %>%
  {wordcloud(.$word, .$n, max.words = 100, scale = c(3, 0.5))}
```

Además de estudiar la frecuencia de aparición de las palabras, podemos hacer lo mismo con grupos de
palabras. Antes vimos las palabras que más aparecían en todo el libro, vayamos ahora un paso más
lejos. Calculemos las parejas de palabras que más aparecen en cada parte del libro. Si nos hemos
leido el libro, este tipo de análisis, que aunque vaya un paso más allá que el anterior sigue siendo
bastante básico, sí podría llevarnos a reconocer elementos distintivos de cada una de las partes.

```{r}
HardTimesBooksBigrams <- HardTimesLines %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>% 
  unite(bigram, word1, word2, sep = " ") %>%
  filter(bigram != "NA NA") %>%
  count(book, bigram, sort = TRUE)

HardTimesBooksBigrams %>% filter(book == 1) %>% mutate(bigram = reorder(bigram, n)) %>% slice(1:19) %>% ggplot(aes(x = n, y = bigram)) + geom_col() + labs(title = "Primer libro")
```

La primera parte podría considerarse una presentación de los diversos personajes que irán
desarrollándose a lo largo de la novela. Por eso mismo vemos muchos nombres propios como las parejas
de palabras que más aparecen. Aquí nos cuenta el autor principalmente la vida de una familia
reconocida en una ciudad de Inglaterra en el comienzo de la industrialización. Se centra en los
hijos mayores del señor Gradgrind, Louisa y Tom. Su vida (educados con una ideología extremadamente
realista y práctica, dejando de lado toda imaginación o fantasía) contrasta con la de la gente de un
circo que llega a la ciudad.

También vemos los pares "eminently practical" y "practical friend", pares de palabras que utilizan
mucho el señor Gradgrind y Bounderby para referirse a prácticamente a cualquier persona cercana y a
su propia amistad.

```{r}
HardTimesBooksBigrams %>% filter(book == 2) %>% mutate(bigram = reorder(bigram, n)) %>% slice(1:19) %>% ggplot(aes(x = n, y = bigram)) + geom_col() + labs(title = "Segundo libro")
```

En la segunda parte del libro los niños ya se han hecho mayores. Tom empieza a trabajar y su hermana
se casa con el senor Bounderby, mucho más mayor que ella, con la misma ideología que su padre, y muy
duro de mollera. El autor trata de enseñarnos como los niños ya crecidos han cesado en su ansia de
fantasías, cuando en la primera parte del libro este era el tema central. Durante esta y la ultima
parte vemos en la evolución de los personajes las consecuencias de esa tan dura eduación.

En esta segunda parte aparecen nuevos personajes, como el señor Harthouse. También podemos ver en
los pares de palabras que más aparecen, la pareja "gold spoon". Esta es parte de una retahíla que
repite constantemente el señor Bounderby a lo largo del libro para referirse al supuesto ansia de
los obreros por querer todo lo que no tienen, y nunca estar satisfechos, según este personaje. Todos
estos diálogos contrastan con la forma de ser y la historia del obrero que nos presentan,
representando todo lo contrario.

```{r}
HardTimesBooksBigrams %>% filter(book == 3) %>% mutate(bigram = reorder(bigram, n)) %>% slice(1:19) %>% ggplot(aes(x = n, y = bigram)) + geom_col() + labs(title = "Tercer libro")
```

Las deudas que acumula Tom por su mal hábito de apostar a juegos de azar acaban llevándole a trazar
un plan (bastante bien elaborado, aquí vemos como su educación tan práctica y realista acabó
sirviéndole para delinquir) para robar el mismo banco en el que trabaja. En la última parte vemos
las consecuencias del robo, que a pesar de estar muy bien trazado el plan, acaba saliendo mal y
perjudicando a muchos otros. Por eso vemos que el par "bank robbery" se encuentra entre los pares de
palabras más frecuentes. También aparece "chimney piece", en el libro se habla también mucho de la
cantidad de chimeneas humeantes que hay en la ciudad, del ambiente tétrico que resulta y de lo malo
o bueno (según a quien le interese) del humo en los pulmones de los habitantes.

### Sentimiento
#### Palabras negativas y positivas que más aparecen
El análisis de sentimiento que llevaremos a cabo será el más sencillo posible. La librería
`tidytext` proporciona un diccionario con información acerca de si una palabra es negativa o
positiva. Solamente esa información, dos categorías. Obviamente no podemos decir de todas las
palabras si tienen una connotación positiva o negativa, no todas las palabras del texto aparecerán
en el diccionario. Para construir una estructura de datos con las palabras del libro positivas y
negativas recurrimos otra vez a la librería `dplyr`, a la función `semi_join()` concretamente.

```{r}
# Seleccionamos el diccionario de palabras a usar
sentiments <- get_sentiments("bing")
positive <- filter(.data = sentiments, sentiment == "positive")
WORDSpositive <- semi_join(x = WORDS, y = positive)
head(WORDSpositive, 25)
negative <- filter(.data = sentiments, sentiment == "negative")
WORDSnegative <- semi_join(x = WORDS, y = negative)
head(WORDSnegative, 25)
```

Veamos las 25 palabras positivas y negativas que más aparecen en el libro. Para visualizar esta
información recurrimos una vez más a un histograma, pero con una diferencia con respecto al
anterior, ahora tenemos las palabras clasificadas en dos grupos. Como estamos interesados en un
análisis de sentimiento, escogeremos las palabras que aparezcan más veces que cierto umbral, el
mismo para las positivas y las negativas. Así podremos ver si hay más palabras de un tipo u otro.
Para evaluar el sentimiento, consideramos éste simplemente como las veces que aparece cada palabra,
y si esta palabra es negativa le cambiaremos el signo. A continuación, representaremos este
histograma.

```{r}
WORDSsentiment <- full_join(x = WORDS, y = sentiments)
WORDSsentiment <- drop_na(WORDSsentiment)

head(WORDSsentiment, 10)

WORDSsentiment %>% filter(n > 20) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribución al sentimiento")
```

Como vemos en el gráfico, predominan claramente las palabras con connotación positiva. Esto nos
podría llevar a pensar que el libro cuenta una historia feliz. Y nada más lejos de la realidad, en
este caso es justo al contrario. Este resultado es consecuencia de la forma de hablar de la época
(educada, elegante y por decirlo de alguna forma, de una forma pomposa) exagerada hasta el extremo,
sobre todo en ciertos personajes.

La información expuesta anteriormente se puede representar también con una nube de palabras, en que
se distinga entre palabras positivas y negativas por color (y localización).

```{r}
HardTimesWords %>%
  inner_join(sentiments) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100, scale = c(3, 0.5))
```

A simple vista, vemos que dominan las palabras positivas, y cuales son las más frecuentes. Pero si
tratamos por ejemplo de ver cuáles son las palabras negativas que más aparecen tendremos serias
dificultades. Aquí vemos claramente porqué el histograma es una representación mucho más completa,
útil, y fácilmente interpretable.

### Evolución del sentimiento
#### Por libros
Tal y como hemos definido el sentimiento (diferencia del contaje de las palabras negativas y
positivas normalizado a la suma de estas dos) podemos calcularlo a intervalos a lo largo del libro y
estudiar su evolución. Este es un ejercicio muy recurrente en las fuentes bibliográficas [1] y [2].
Para empezar, veamos el sentimiento de cada parte del libro con un histograma.

```{r}
HardTimesSentiment <- inner_join(x = HardTimesWords, y = sentiments)

BooksSentiment <- count(x = HardTimesSentiment, book, sentiment)

p <- filter(.data = BooksSentiment, sentiment == "negative")
n <- filter(.data = BooksSentiment, sentiment == "positive")
BooksSentiment <- data.frame(book = p$book, sentiment = (p$n - n$n)/(p$n + n$n))

ggplot(BooksSentiment, aes(x = book, y = sentiment)) + geom_bar(stat = "identity", show.legend = FALSE)
```

Si recordamos el pequeño resumen del libro expuesto anteriormente, es difícil interpretar este
resultado. Teniendo en cuenta el contenido del libro, cabría esperar que el sentimiento a lo largo
de las partes del libro tuviera una tendencia negativa, ya que poco a poco nos vamos acercando al
triste final, y las tragedias y consecuencias negativas de todo lo anterior van apareciendo. Aunque
sí es cierto que no todo hacia el final es malo, vemos que algunos personajes se dan cuenta de sus
errores (aunque ya sea tarde) e intentan cambiar, sí es cierto que no sabemos interpretar el
histograma anterior. Esto es debido a la capacidad limitada de estas técnicas de análisis,
recordemos que solo hemos visto si las palabras individualmente tienen una connotación positiva o
negativa. Esto no es suficiente para un obra literaria con un lenguaje complejo, harían falta
técnicas más complicadas que están fuera del alcance y los objetivos de este trabajo.

#### Por capítulos
Veamos ahora la evolución del sentimiento a lo largo de los capítulos.

```{r}
ChaptersSentiment <- list(length(BooksStart))
for (i in 1:length(BooksStart)) {
  ChaptersSentiment[[i]] <- count(x = HardTimesSentiment[HardTimesSentiment$book == i, ], chapter, sentiment)
}

for (i in 1:length(BooksStart)) {
  p <- filter(.data = ChaptersSentiment[[i]], sentiment == "negative")
  n <- filter(.data = ChaptersSentiment[[i]], sentiment == "positive")
  ChaptersSentiment[[i]] <- data.frame(book = i, chapter = p$chapter, sentiment = (p$n - n$n)/(p$n + n$n))
}

ChaptersSentiment <- bind_rows(ChaptersSentiment)

ggplot(ChaptersSentiment, aes(x = chapter, y = sentiment, col = book)) + geom_bar(stat = "identity", show.legend = FALSE) + facet_wrap( ~ book, ncol = 1, scales = "free_x")
```

Tal y como cabría esperar, cuanto más pequeño sea el intervalo a lo largo del cual calculamos el
sentimiento, más variará el mismo. Para ver si ahora se corresponden mejor los resultados con la
historia contada en el libro haría falta profundizar mucho más en la historia, así que no llegaremos
a este nivel de detalle. Como tendencia más o menos general podemos decir que el sentimiento va
oscilando a lo largo de la novela.

Fijémonos en los últimos capítulos. Al final del libro, como ya hemos comentado, se rompen las vidas
de los personajes y se hacen palpables las consecuencias de su vida anterior y su educación. Esto
podemos verlo en el antepenúltimo y en el penúltimo capítulo. En el último capítulo, como ya muchas
historias nos tienen acostumbrados, a pesar de todas las desgracias anteriores, se cuenta la vida
posterior a la historia de los personajes principales. Aunque muchos detalles sean tristes y
trágicos, la tendencia general es positiva. Podemos observar que estos detalles han sido detectados
más o menos bien por el análisis de sentimiento.

#### Por cada 100 palabras
También podemos ver la evolución del sentimiento a lo largo del texto por intervalos más regulares.
Veamos la evolución del sentimiento por cada 100 palabras.

```{r}
Sentiment <- list(nrow(HardTimesSentiment) %% 100 + 2)

Group100Words <- seq(from = 101, to = nrow(HardTimesSentiment), by = 100)

for (i in Group100Words) {
  Sentiment[[(i - 1)/100]] <- slice(HardTimesSentiment, (i - 100):(i - 1))
}

Sentiment[[(i - 1)/100 + 1]] <- slice(HardTimesSentiment, i:nrow(HardTimesSentiment))

for (i in 1:length(Group100Words)) {
  Sentiment[[i]] <- count(x = Sentiment[[i]], sentiment)
  Sentiment[[i]] <- Sentiment[[i]]$n[Sentiment[[i]]$sentiment == "positive"] - Sentiment[[i]]$n[Sentiment[[i]]$sentiment == "negative"]
}

l <- length(Group100Words) + 1
Sentiment[[l]] <- count(x = Sentiment[[l]], sentiment)
Sentiment[[l]] <- Sentiment[[l]]$n[Sentiment[[l]]$sentiment == "positive"] - Sentiment[[l]]$n[Sentiment[[l]]$sentiment == "negative"]

Group10LinesSentiment <- data.frame(Group100Words = seq(length(Group100Words) + 1), sentiment = unlist(Sentiment))

ggplot(Group10LinesSentiment, aes(x = Group100Words, y = sentiment)) + geom_bar(stat = "identity")
```

Podría sorprendernos la cantidad escasa de puntos del histograma, pero recordemos, no todas las
palabras tienen connotación positiva o negativa, estamos viendo un subconjunto de las palabras del
libro. Tal y cómo pasaba en la evolución por capítulos, la tendencia general en el sentimiento son
oscilaciones de amplitud variable. Como ahora hemos tomado más intervalos, las variaciones son más
abruptas. En estos picos significativos predominan las palabras positivas o negativas, sí, pero para
ver si realmente esto se corresponde con el significado del texto tendríamos que volver a leer estas
partes de la novela.

Lo lógico sería que cuanto menor sea el intervalo a lo largo del que calculamos el sentimiento,
menos se corresponda con el significado del texto. Pero como vimos al promediar por libros, tampoco
podemos hacer el intervalo demasiado grande, ya que estaríamos juntando partes negativas y
positivas. Habría que buscar el equilibrio para dar con un intervalo que reproduzca de mejor forma
la connotación del texto a partir de un análisis por palabras individuales únicamente.

## Redes de co-ocurrencia
Las redes de co-ocurrencia nos cuentan de forma visual las relaciones de una palabra con las demás a
lo largo de un texto. Son representaciones de las palabras con un tamaño mayor cuántas más veces
aparezcan, pero a diferencia de las nubes de palabras, la disposición de las palabras aquí no es
arbitraria. En la representación aparecen interconectadas por líneas las palabras. De esta forma un
mapa de co-ocurrencia nos dice las relaciones existentes en un texto entre una palabras y las demás,
pero también cómo las demás se relacionan entre ellas. Tal y como comentamos en la introducción, en
el análisis de un libro lo más interesante que se nos ocurre de primeras es contruir este tipo de
redes para los personajes principales. Esto es justo lo que vamos a hacer.

En la elaboración de las representaciones que hemos visto hasta el momento la tarea más larga ha
sido el preprocesamiento de los datos, adecuándolos para cada representación gráfica. Pero una vez
tenemos preparada la información necesaria, los diversos paquetes que hemos introducido han
realizado prácticamente todo el trabajo. Aunque también es cierto que representar en un histograma
las palabras más frecuentes, o analizar su sentimiento según diccionarios ya hechos, no es algo que a
priori parezca complicado. En este caso tendremos que trabajar mucho más. La búsqueda de las
relaciones entre una palabra y las demás involucra el cálculo de matrices de covarianza y otras
cosas más que iremos explicando.

Aun así, ahora también recurrimos a otra librería que nos ayudará considerablemente, `quanteda`.
Este paquete incluye un tipo especial de objeto `corpus`, con un texto, y campos informativos
adicionales sobre variables tipo documento de los que no haremos uso en este trabajo. También
emplearemos la librería `lexicon`, con diccionarios que necesitaremos para contar como únicas
ciertas palabras y abreviaturas y para eliminar las palabras vacías.

Comencemos. Lo primero es dividir el texto original en oraciones. Para ello tenemos que juntar antes
todas las líneas del mismo, ya que hasta la función empleada para descargar el libro lo almacenaba
línea a línea. 

```{r}
# Creamos una variable corpus con todo el texto del libro junto
HardTimesCorpus <- corpus(paste0(list(HardTimesLines$text)))
# Lo dividimos en oraciones,
corpus_sentences <- corpus_reshape(x = HardTimesCorpus, to = "sentences")
# Diccionario de lemas
data(hash_lemmas)
# Diccionario de palabras vacías
data(sw_jockers)

# Preprocesado del corpus de palabras
corpus_tokens <- corpus_sentences %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_replace(hash_lemmas$token, hash_lemmas$lemma, valuetype = "fixed") %>% 
  tokens_remove(pattern = sw_jockers, padding = T)
```

Estamos interesados en las palabras que aparecen en cada oración, pero no en si aparecen más de una
vez en una oración. Es decir, buscamos ver con qué palabras se relaciona cada palabra. Esta
información la podemos codificar en forma de ceros (la palabra no aparece) y unos (la palabra sí
aparece). Además, solo tendremos en cuenta las palabras que aparecen en más de 10 oraciones.

```{r}
minimumFrequency <- 10
# Creación del DTM, recortado del vocabulario y fijado de valores binarios para la
# presencia o absencia de tipos
binDTM <- corpus_tokens %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency) %>% 
  dfm_weight("boolean")
```

Para calcular la ocurrencia conjunta de palabras solo tenemos que multiplicar la matriz anterior
traspuesta por ella misma. Notemos que el resultado no será una medida exacta de las frecuencias
(veces que aparece una palabra por oración), será una especie de distancia (frecuencias al
cuadrado). Es fácil entender la matriz si reducimos su dimensión al máximo posible, una matriz
cuadrada 2x2 que represente dos oraciones y dos palabras. En la diagonal tendremos los términos
correspondientes a cada palabra en el conjunto de las dos oraciones. Fuera de la diagonal tendremos
información sobre si las palabras aparecen juntas o no en las oraciones. Es una matriz simétrica.

```{r}
# Multiplicación de matrices para contaje de co-ocurrencias
coocCounts <- t(binDTM) %*% binDTM
as.matrix(coocCounts[202:205, 202:205])
```

Como hemos codificado la aparición de una palabra en la matriz inicial como 1, la diagonal sí es
exactamente el número de oraciones en las que aparece cada palabra. Y como hemos codificado con un 0
el que una palabra no aparezca, cada elemento de fuera de la diagonal es el número de oraciones en
las que aparecen juntas las dos palabras correspondientes.

Hasta aquí podemos entender el proceso, lo que sigue se escapa de los objetivos del trabajo, así que
solo esplicaremos qué queremos hacer y no cómo. Hemos visto cómo calcular la ocurrencia conjunta de
las palabras en grupos, pero para establecer relaciones más o menos fuertes entre las mismas hace
falta estudiar la significación de la co-ocurencia. En la referencia [3] encontramos más información
al respecto.

Construyamos el mapa de co-ocurrencia del señor Bounderby y de la señora Sparsit, una viuda que un
día fue una dama de alta alcurnia y que ahora ha acabado sirviendo al realista, tozudo y orgulloso
señor Bounderby.

```{r}
# Leemos el siguiente código fuente para realizar el cálculo de la co-ocurrencia
source("calculateCoocStatistics.R")
# Definimos el parámetro para la representación de las co-ocurrencias de un concepto
numberOfCoocs <- 15
# Determinamos el término del cual se han de medir co-competidores
coocTerm <- "bounderby"
```

```{r}
coocs <- calculateCoocStatistics(coocTerm, binDTM, measure = "LOGLIK")
# Enseñamos los términos principales de numberofCoocs
print(coocs[1:numberOfCoocs])
```

```{r}
resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# La estructura del objeto temporal de la gráfica es idéntico al de resultGraph
tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Rellenamos el data frame de forma que producimos el número correcto de líneas
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]
# Fijamos el término de búsqueda en la primera columna en todas las líneas
tmpGraph[, 1] <- coocTerm
# Fijamos las co-ocurrencias en la segunda columna de su respectiva línea
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]
# Fijamos las significancias
tmpGraph[, 3] <- coocs[1:numberOfCoocs]

# Unimos los triples a resultGraph
resultGraph <- rbind(resultGraph, tmpGraph)

# Iteramos sobre los elementos de numberOfCoocs más significativos
for (i in 1:numberOfCoocs) {
  # Hacemos el cálculo de co-ocurrencia para el término i-ésimo de las co-ocurrencias
  # de los términos de búsqueda
  newCoocTerm <- names(coocs)[i]
  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure = "LOGLIK")
  
  # Imprimimos las co-ocurrencias
  coocs2[1:10]
  
  # Estructura del objeto del gráfico temporal
  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
  tmpGraph[, 1] <- newCoocTerm
  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
  
  # Unimos el resultado al gráfico resultante
  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}

# Muestreo de algunos ejemplos de resultGraph
resultGraph[sample(nrow(resultGraph), 6), ]
```

```{r}
# Establecemos la semilla para el gráfico
set.seed(1)

# Creamos el objeto de gráfico como un gráfico sin dirección
graphNetwork <- graph.data.frame(resultGraph, directed = F)

# Identificamos todos los nodos con menos de 2 bordes
verticesToRemove <- V(graphNetwork)[degree(graphNetwork) < 2]
# Estos bordes se quitan del gráfico
graphNetwork <- delete.vertices(graphNetwork, verticesToRemove) 

# Asignamos colores a nodos (término en azul, todo lo otro en naranja)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, "cornflowerblue", "orange") 

# Fijamos el color de los bordes
E(graphNetwork)$color <- adjustcolor("DarkGray", alpha.f = .5)
# Significancia de escala entre 1 y 10 para el ancho del borde
E(graphNetwork)$width <- scales::rescale(E(graphNetwork)$sig, to = c(1, 10))

# Fijamos los bordes de acuerdo al radio
E(graphNetwork)$curved <- 0.15 
# Fijamos el tamaño de los nodos de acuerdo con su grado de conexión (entre 5 y 15)
V(graphNetwork)$size <- scales::rescale(log(degree(graphNetwork)), to = c(5, 15))

# Definimos el marco y el espaciado del gráfico
par(mai = c(0, 0, 1, 0)) 

# Gráfico final
plot(graphNetwork,
     # Configuramos la plantilla del gráfico
     layout = layout.fruchterman.reingold,
     main = paste0(coocTerm, "Graph"),
     vertex.label.family = "sans",
     vertex.label.cex = 0.8,
     vertex.shape = "circle",
     # Movemos un poco las etiquetas de los nodos
     vertex.label.dist = 0.5,
     vertex.frame.color = adjustcolor("darkgray", alpha.f = .5),
     vertex.label.color = "black",
     vertex.label.font = 2,
     # Nombres de los nodos
     vertex.label = V(graphNetwork)$name,
     # Tamaño de la fuente de los nombres de los nodos
     vertex.label.cex = 1)
```

# Grandes Esperanzas
Esta novela narra la historia de Phillip Pirrip, un huérfano aprendiz de herrero cuya aspiración
pasará a ser convertirse en un noble caballero, describiendo su vida desde su niñez hasta su
madurez. Considerada una novela de aprendizaje, la historia puede también ser considerada como una
semiautobiografía de Dickens, al igual que muchas de sus obras, en la cual mezcla sus experiencias
de vida con su entorno social. La trama de la historia toma desde la víspera de Navidad de 1812,
cuando el protagonista tiene solo siete años de edad, hasta el invierno de 1840. [4]

## Organización de los datos
### Clasificación de las líneas por capítulos
Veamos qué nos depara este libro. Está dividido en 59 capítulos, sin estar dividido por libros. Lo
trataremos como un solo libro, aunque sí respetaremos su división por capítulos. La obra en sí
comienza más abajo de lo que descargamos de Gutenberg, dado que se incluye una lista de capítulos
aquí, así que hemos de quitar estas líneas para poder clasificar las líneas por capítulos
adecuadamente.

```{r}
# Descargamos Great Expectations de la biblioteca Gutenberg,
GreatExpectationsRaw <- gutenberg_download(1400)

# Nos deshacemos de la columna llamada "gutenberg_id", ya que no nos sirve para nada,
GreatExpectationsRaw <- subset(GreatExpectationsRaw, select = -gutenberg_id)

# Quitamos las primeras líneas del texto, dado que contienen una lista
# de capítulos, que no nos ayudan para dividir el texto en capítulos
GreatExpectationsRaw <- GreatExpectationsRaw[-c(1:(14 + 59 + 6)),]

# Así es como haremos para saber cuándo comienza cada capítulo;
# Afortunadamente, la palabra "Chapter" no aparece en la novela
# en sí, únicamente se usa para separar los capítulos.
# Entonces, podemos usar grep() para saber en qué líneas
# comienzan los capítulos.
# Sumamos un 1 dado que no queremos incluir a la línea que contiene "Chapter"
# en nuestra clasificación.
GEChaptersStart <- grep(pattern = "Chapter", x = GreatExpectationsRaw$text) + 1

# Los capítulos acaban cuando comienza otro, así que usaremos
# el vector anterior para saber esto,
GEChaptersEnd <- integer(length = length(GEChaptersStart))
for (i in 1:length(GEChaptersStart)) {
  if (i == length(GEChaptersStart)) {
    GEChaptersEnd[i] <- length(GreatExpectationsRaw$text)
    next
  }
  GEChaptersEnd[i] <- GEChaptersStart[i + 1] - 2
}

GEChapter <- numeric(length(GreatExpectationsRaw$text))
GEText <- character(length(GreatExpectationsRaw$text))

m <- 1
# Dividimos en capítulos
for (i in 1:length(GEChaptersStart)) {
  s <- GEChaptersStart[i]
  e <- GEChaptersEnd[i]
  # En cada elemento de "GEChapter" se indica a qué capítulo
  # corresponde cada línea,
  GEChapter[m:(e - s + m)] <- i
  # Y en "GEText" van las líneas en sí.
  GEText[m:(e - s + m)] <- GreatExpectationsRaw$text[s:e]
  m <- m + e - s + 1
}

# Última palabra del libro
GELastWordIndex <- which(GEText == "from her.")
# Juntamos la información del capítulo al que pertenece cada línea y las líneas
# en sí en un data frame,
GreatExpectationsLines <- data.frame(chapter = GEChapter[1:GELastWordIndex], text = GEText[1:GELastWordIndex])
# Y guardamos el libro dividido según líneas y capítulos en un fichero,
save(GreatExpectationsLines, file = "data/GreatExpectationsLines.Rdata")
```

### División por palabras
De nuevo, dividimos el libro en palabras, o lo que es lo mismo, "tokenizar" las líneas
leídas según los espacios (" ") que se encuentren. En cada fila, guardamos una palabra,
junto con información sobre el capítulo donde se encuentran, como antes.

```{r}
GEWordsRaw <- vector("list", length = nrow(GreatExpectationsLines))
GEWords <- vector("list", length = nrow(GreatExpectationsLines))

# Creamos tres vectores con espacio de sobra para almacenar
# datos sobre cada palabra, no necesitamos saber exactamente
# cuántas hay, al no importar tanto el uso de memoria.
chapter <- numeric(10*nrow(GreatExpectationsLines))
line <- numeric(10*nrow(GreatExpectationsLines))
word <- character(10*nrow(GreatExpectationsLines))

n <- 0
for (i in 1:nrow(GreatExpectationsLines)) {
  # En cada elemento de GEWordsRaw, partimos las líneas de acuerdo con estos
  # caracteres especificados en "split",
  GEWordsRaw[[i]] <- strsplit(x = GreatExpectationsLines$text[i], split = "[ .,;”—)(?!‘:]")
  # Y en GEWords almacenamos GEWordsRaw sin los elementos que no contengan palabras,
  GEWords[[i]] <- GEWordsRaw[[i]][[1]][!GEWordsRaw[[i]][[1]] == ""]
  if (length(GEWords[[i]]) == 0) {
    next
  }
  l <- length(GEWords[[i]])
  n <- n + l
  chapter[(n - l + 1):n] <- GreatExpectationsLines$chapter[i]
  line[(n - l + 1):n] <- i
  word[(n - l + 1):n] <- GEWords[[i]]
}

word <- tolower(word)
GreatExpectationsWords <- data.frame(book = book[1:n], chapter = chapter[1:n], line = line[1:n], word = word[1:n])
GreatExpectationsWords <- GreatExpectationsWords[GreatExpectationsWords$word != "’", ]
save(GreatExpectationsWords, file = "data/GreatExpectationsWords.Rdata")
```

## Visualización de características
### Palabras más frecuentes
Veamos las 25 palabras más frecuentes en "Grandes Esperanzas", para luego representar esto
gráficamente.

```{r}
GEWords <- count(GreatExpectationsWords, word, sort = TRUE) %>%
  anti_join(get_stopwords())
# Quitamos una cierta palabra, dado que no parece tener sentido...
str(GEWords)
subset(GEWords, GEWords$word != "“i")
head(GEWords, 25)
```

```{r}
GEWords %>% filter(n > 180) %>% mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + coord_flip() + labs(title = "Palabras más comunes en Grandes Esperanzas")
```

```{r}
GreatExpectationsWords %>%
  anti_join(stop_words) %>%
  count(word) %>%
  {wordcloud(.$word, .$n, max.words = 100, scale = c(3, 0.5))}
```

De nuevo, representamos gráficamente la frecuencia de aparición de bigramas,

```{r}
GreatExpectationsBigrams <- GreatExpectationsLines %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>% 
  unite(bigram, word1, word2, sep = " ") %>%
  filter(bigram != "NA NA") %>%
  count(bigram, sort = TRUE)

GreatExpectationsBigrams %>% mutate(bigram = reorder(bigram, n)) %>% slice(1:19) %>% ggplot(aes(x = n, y = bigram)) + geom_col() + labs(title = "Bigramas más comunes en Grandes Esperanzas")
```

### Sentimiento
#### Palabras negativas y positivas más frecuentes
Veamos las 25 palabras positivas y negativas que más aparecen en el libro.

```{r}
# Seleccionamos el diccionario de palabras a usar
sentiments <- get_sentiments("bing")
```

```{r}
positive <- filter(.data = sentiments, sentiment == "positive")
GEWordsPositive <- semi_join(x = GEWords, y = positive)
head(GEWordsPositive, 25)
negative <- filter(.data = sentiments, sentiment == "negative")
GEWordsNegative <- semi_join(x = GEWords, y = negative)
head(GEWordsNegative, 25)
```

Como antes, primero visualizamos el contaje de la ocurrencia de cada palabra mediante un histograma
dispuesto verticalmente, con columnas apuntando en direcciones opuestas y con colores escogidos
adecuadamente de forma que no sólo enseñen a primera vista que tipo de dato categórico representan,
sino que tenemos el valor añadido de que el azul y el naranja son colores contrarios en el círculo
cromático, facilitando así aún más la legibilidad de esta gráfica.

```{r}
GEWordsSentiment <- full_join(x = GEWords, y = sentiments)
GEWordsSentiment <- drop_na(GEWordsSentiment)

head(GEWordsSentiment, 10)

GEWordsSentiment %>% filter(n > 20) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribución al sentimiento")
```

En contraste a "Tiempos Difíciles", el sentimiento de este libro parece estar equilibrado entre
positivo y negativo, aunque teniendo en cuenta el lenguaje de la época, como antes, es bastante
posible que este libro sea incluso más negativo que el anterior, algo plausible dado que esta
novela tiene tintes autobiográficos. Sin embargo, si nos fijamos un poco mejor en la tendencia
que tienen los bines del histograma para cada sentimiento, vemos que los negativos parecen ser
más pequeños en general que los positivos, pero están dominados por "miss", que tiene un contaje
superior a cualquier término aquí visto.

Esto es debido a que está codificado de forma algo errónea; "miss" puede significar "echar de menos"
pero también significa "señorita"; podemos decir que tenemos ruido en la medida de su frecuencia.
Para deshacernos del ruido y cuantificar adecuadadmente el sentimiento, haría falta analizar
el contexto en el que ha sido mencionada esta palabra, cosa que queda fuera de los objetivos
de este trabajo. Aún así, la visualización del sentimiento nos ha permitido ver una anomalía
en el procesado del texto, lo cual resulta de suma importancia al hacer análisis más profundos
para refinar el código y/o las técnicas utilizadas.

Como antes, podemos representar la anterior información mediante una nube de palabras
de una forma idéntica, aplicando las mismas palabras.

```{r}
GreatExpectationsWords %>%
  inner_join(sentiments) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100, scale = c(3, 0.5))
```

Se puede ver, de nuevo, que las palabras positivas tienen una mayor frecuencia en general,
y la forma en que destaca "miss".

### Evolución del sentimiento
De nuevo, calcularemos la evolución de la frecuencia de los dos sentimientos a lo largo
de dos divisiones del texto; según capítulos y cada 100 palabras.

#### Por capítulos

```{r}
GreatExpectationsSentiment <- inner_join(x = GreatExpectationsWords, y = sentiments)
ChaptersSentiment <- count(x = GreatExpectationsSentiment, chapter, sentiment)

p <- filter(.data = ChaptersSentiment, sentiment == "negative")
n <- filter(.data = ChaptersSentiment, sentiment == "positive")
ChaptersSentiment <- data.frame(chapter = p$chapter, sentiment = (p$n - n$n)/(p$n + n$n))

ChaptersSentiment <- bind_rows(ChaptersSentiment)
ggplot(ChaptersSentiment, aes(x = chapter, y = sentiment)) + geom_bar(stat = "identity", show.legend = FALSE)
```

Como antes, cuanto más pequeño sea el intervalo a lo largo del cual calculamos el
sentimiento, más variará el mismo.

#### Por cada 100 palabras
También podemos ver la evolución del sentimiento a lo largo del texto por intervalos más regulares.
Veamos la evolución del sentimiento por cada 100 palabras.

```{r}
Sentiment <- list(nrow(GreatExpectationsSentiment) %% 100 + 2)

Group100Words <- seq(from = 101, to = nrow(GreatExpectationsSentiment), by = 100)

for (i in Group100Words) {
  Sentiment[[(i - 1)/100]] <- slice(GreatExpectationsSentiment, (i - 100):(i - 1))
}

Sentiment[[(i - 1)/100 + 1]] <- slice(GreatExpectationsSentiment, i:nrow(GreatExpectationsSentiment))

for (i in 1:length(Group100Words)) {
  Sentiment[[i]] <- count(x = Sentiment[[i]], sentiment)
  Sentiment[[i]] <- Sentiment[[i]]$n[Sentiment[[i]]$sentiment == "positive"] - Sentiment[[i]]$n[Sentiment[[i]]$sentiment == "negative"]
}

l <- length(Group100Words) + 1
Sentiment[[l]] <- count(x = Sentiment[[l]], sentiment)
Sentiment[[l]] <- Sentiment[[l]]$n[Sentiment[[l]]$sentiment == "positive"] - Sentiment[[l]]$n[Sentiment[[l]]$sentiment == "negative"]

Group10LinesSentiment <- data.frame(Group100Words = seq(length(Group100Words) + 1), sentiment = unlist(Sentiment))

ggplot(Group10LinesSentiment, aes(x = Group100Words, y = sentiment)) + geom_bar(stat = "identity")
```

## Co-occurrence maps

```{r}
GreatExpectationsCorpus <- corpus(paste0(list(GreatExpectationsLines$text)))
corpus_sentences <- corpus_reshape(x = GreatExpectationsCorpus, to = "sentences")
# Diccionario de lemas
data(hash_lemmas)
# Palabras de parada
data(sw_jockers)

# Preprocessing of the corpus of sentences
corpus_tokens <- corpus_sentences %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_replace(hash_lemmas$token, hash_lemmas$lemma, valuetype = "fixed") %>% 
  tokens_remove(pattern = sw_jockers, padding = T)
```

```{r}
minimumFrequency <- 10
# Creación del DTM, recortado del vocabulario y fijado de valores binarios para la
# presencia o absencia de tipos
binDTM <- corpus_tokens %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency) %>% 
  dfm_weight("boolean")
```

```{r}
# Multiplicación de matrices para contaje de co-ocurrencias
coocCounts <- t(binDTM) %*% binDTM
as.matrix(coocCounts[1:3, 1:3])
```

```{r}
# Leemos el siguiente código fuente para realizar el cálculo de la co-ocurrencia
source("calculateCoocStatistics.R")
# Definimos el parámetro para la representación de las co-ocurrencias de un concepto
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
coocTerm <- "pip"
```

```{r}
coocs <- calculateCoocStatistics(coocTerm, binDTM, measure = "LOGLIK")
# Enseñamos los términos principales de numberofCoocs
print(coocs[1:numberOfCoocs])
```

```{r}
resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# La estructura del objeto temporal de la gráfica es idéntico al de resultGraph
tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Rellenamos el data frame de forma que producimos el número correcto de líneas
tmpGraph[1:numberOfCoocs, 3] <- coocs[1:numberOfCoocs]
# Fijamos el término de búsqueda en la primera columna en todas las líneas
tmpGraph[, 1] <- coocTerm
# Fijamos las co-ocurrencias en la segunda columna de su respectiva línea
tmpGraph[, 2] <- names(coocs)[1:numberOfCoocs]
# Fijamos las significancias
tmpGraph[, 3] <- coocs[1:numberOfCoocs]

# Unimos los triples a resultGraph
resultGraph <- rbind(resultGraph, tmpGraph)

# Iteramos sobre los elementos de numberOfCoocs más significativos
for (i in 1:numberOfCoocs){
  # Hacemos el cálculo de co-ocurrencia para el término i-ésimo de las co-ocurrencias
  # de los términos de búsqueda
  newCoocTerm <- names(coocs)[i]
  coocs2 <- calculateCoocStatistics(newCoocTerm, binDTM, measure = "LOGLIK")
  
  # Imprimimos las co-ocurrencias
  coocs2[1:10]
  
  # Estructura del objeto del gráfico temporal
  tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  tmpGraph[1:numberOfCoocs, 3] <- coocs2[1:numberOfCoocs]
  tmpGraph[, 1] <- newCoocTerm
  tmpGraph[, 2] <- names(coocs2)[1:numberOfCoocs]
  tmpGraph[, 3] <- coocs2[1:numberOfCoocs]
  
  # Unimos el resultado al gráfico resultante
  resultGraph <- rbind(resultGraph, tmpGraph[2:length(tmpGraph[, 1]), ])
}

# Muestreo de algunos ejemplos de resultGraph
resultGraph[sample(nrow(resultGraph), 6), ]
```

```{r}
# Establecemos la semilla para el gráfico
set.seed(1)

# Creamos el objeto de gráfico como un gráfico sin dirección
graphNetwork <- graph.data.frame(resultGraph, directed = F)

# Identificamos todos los nodos con menos de 2 bordes
verticesToRemove <- V(graphNetwork)[degree(graphNetwork) < 2]
# Estos bordes se quitan del gráfico
graphNetwork <- delete.vertices(graphNetwork, verticesToRemove) 

# Asignamos colores a nodos (término en azul, todo lo otro en naranja)
V(graphNetwork)$color <- ifelse(V(graphNetwork)$name == coocTerm, "cornflowerblue", "orange") 

# Fijamos el color de los bordes
E(graphNetwork)$color <- adjustcolor("DarkGray", alpha.f = .5)
# Significancia de escala entre 1 y 10 para el ancho del borde
E(graphNetwork)$width <- scales::rescale(E(graphNetwork)$sig, to = c(1, 10))

# Fijamos los bordes de acuerdo al radio
E(graphNetwork)$curved <- 0.15 
# Fijamos el tamaño de los nodos de acuerdo con su grado de conexión (entre 5 y 15)
V(graphNetwork)$size <- scales::rescale(log(degree(graphNetwork)), to = c(5, 15))

# Definimos el marco y el espaciado del gráfico
par(mai = c(0, 0, 1, 0)) 

# Gráfico final
plot(graphNetwork,
     # Configuramos la plantilla del gráfico
     layout = layout.fruchterman.reingold,
     main = paste0(coocTerm, "Graph"),
     vertex.label.family = "sans",
     vertex.label.cex = 0.8,
     vertex.shape = "circle",
     # Movemos un poco las etiquetas de los nodos
     vertex.label.dist = 0.5,          
     vertex.frame.color = adjustcolor("darkgray", alpha.f = .5),
     vertex.label.color = "black",
     vertex.label.font = 2,
     # Nombres de los nodos
     vertex.label = V(graphNetwork)$name,      
     # Tamaño de la fuente de los nombres de los nodos
     vertex.label.cex = 1)
```

# Conclusiones
Si bien es cierto que todo el análisis llevado a cabo ha sido realmente interesante e instructivo,
más cuando uno se ha leído el libro, uno podría dudar sobre la utilidad práctica de estas técticas
de procesado y visualización. En nuestra opinión, no cabe la menor duda al respecto, aplicar estas
técnicas a libros es un ejercicio para pasar el rato, pero pueden aplicarse a cualquier tipo de
texto. Si uno tuviera por ejemplo un canal de YouTube con muchos comentarios, y estuviera interesado
en analizarlos, podría aplicar estas técnicas. Y quien dice un canal de YouTube, dice también una
tienda de ropa, de informática, una página web, una aplicación... En definitiva, podríamos aplicar
estas técnicas para el análisis de opiniones de cualquier producto o servicio. Al tener muchas,
éstas nos podrían ayudar a tener una visión general de la opinión de la gente de una forma
automática. Hemos elegido analizar libros porque como ejercicio lo hemos considerado más
entretenido.

# Bibliografía
[1] https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
[2] https://datavizm20.classes.andrewheiss.com/example/13-example/
[3] https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html
[4] https://es.wikipedia.org/wiki/Grandes_esperanzas
